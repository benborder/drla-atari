{
	"environment": {
		"rom_file": "rom.bin",
		"end_episode_on_life_loss": true,
		"clip_reward": false, // clip with the agent instead so the displayed reward is correct
		"frame_skip": 3,
		"noop_reset_max_frames": 10,
		"frame_stack": 4,
		"grayscale": true,
		"output_resolution": [
			84,
			84
		]
	},
	"observation_save_period": 500,
	"observation_gif_save_period": 1000,
	"agent": {
		"asynchronous_env": false,
		"env_count": 8,
		"use_cuda": true,
		"train_algorithm": {
			"train_algorithm_type": "PPO",
			"horizon_steps": 256,
			"total_timesteps": 20000,
			"start_timestep": 0,
			"learning_rate": 0.00025,
			"learning_rate_min": 0.000001,
			"lr_schedule_type": "Linear",
			"lr_decay_rate": 1.0,
			"epsilon": 1e-8,
			"kl_target": 0.03,
			"max_grad_norm": 0.5,
			"num_epoch": 4,
			"num_mini_batch": 4,
			"clip_vf": true,
			"clip_range_vf": 0.1,
			"clip_range_policy": 0.1,
			"entropy_coef": 0.01,
			"policy_loss_coef": 1.0,
			"value_loss_coef": 0.5,
			"gamma": [
				0.996
			],
			"gae_lambda": 0.95
		},
		"rewards": {
			"reward_clamp_min": -1.0,
			"reward_clamp_max": 1.0,
			"combine_rewards": false
		},
		"model": {
			"model_type": "ActorCritic",
			"feature_extractor": {
				"conv_layers": [
					[
						{
							"out_channels": 32,
							"kernel_size": 8,
							"stride": 4,
							"padding": 0
						},
						{
							"out_channels": 64,
							"kernel_size": 4,
							"stride": 2,
							"padding": 0
						},
						{
							"out_channels": 64,
							"kernel_size": 3,
							"stride": 1,
							"padding": 0
						}
					]
				]
			},
			"shared": {
				"name": "shared",
				"layers": [
					{
						"size": 512,
						"activation": "ReLU"
					}
				]
			}
		}
	}
}
